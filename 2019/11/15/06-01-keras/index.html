<!DOCTYPE html>
<html>

  <head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>텍스트 데이터 처리하기 - 순환 신경망</title>
  <meta name="description" content="">

	<!-- CSS & fonts -->
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,500" rel="stylesheet">
  <link rel="stylesheet" href="/css/main.css">

  <!-- Open Graph -->
  <meta property="og:title" content="텍스트 데이터 처리하기 - 순환 신경망">
  <meta property="og:type" content: "website">
  <meta property="og:url" content= "https://thereviewindex.com/blog/2019/11/15/06-01-keras/">
  <meta property="og:description" content="">
  <meta property="og:locale" content= "en_US">
  <meta property="og:site_name" content="The Review Index">
  <meta property="og:image" content="https://thereviewindex.com/blog/">
  <meta property="og:image:url" content="https://thereviewindex.com/blog/">
  <meta property="og:image:secure_url" content="https://thereviewindex.com/blog/">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="1200">
  
  <!-- twitter card -->
  <meta property="twitter:card" content="summary">
  <meta property="twitter:title" content="텍스트 데이터 처리하기 - 순환 신경망">
  <meta property="twitter:description" content="">
  <meta property="twitter:url" content= "https://thereviewindex.com/blog/2019/11/15/06-01-keras/">
  <meta property="twitter:image" content="https://thereviewindex.com/blog/">


	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/png" href="img/favicon.png">


  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  
  ga('create', 'UA-78263144-1', 'auto');
  ga('send', 'pageview');
  </script>
  <!-- End Google Analytics -->

</head>

  <body>
    
    <!-- Facebook Comments Script -->
    <div id="fb-root"></div>
      <script>(function(d, s, id) {
        var js, fjs = d.getElementsByTagName(s)[0];
        if (d.getElementById(id)) return;
        js = d.createElement(s); js.id = id;
        js.src = 'https://connect.facebook.net/en_GB/sdk.js#xfbml=1&version=v2.11&appId=288262758360783';
        fjs.parentNode.insertBefore(js, fjs);
      }(document, 'script', 'facebook-jssdk'));</script>
  
    <div id="wrap">
  	  	
  	  	<!-- Navigation -->
  	  	<nav id="nav">
	<div id="nav-list">
		<a href="/">Home</a>

		<!-- Nav pages -->
	  
	    
	  
	    
	  
	    
	      <a href="/about/" title="About">About</a>
	    
	  
	    
	      <a href="/archive/" title="Archive">Archive</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	      <a href="/ml/" title="Machine Learning">Machine Learning</a>
	    
	  
	    
	      <a href="/paper_review/" title="Paper Review">Paper Review</a>
	    
	  
	    
	      <a href="/projects/" title="Projects">Projects</a>
	    
	  
	    
	  
	    
	      <a href="/study/" title="Study">Study</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
    
    <!-- Nav links -->
	  <!-- <a href="/">Blog</a> -->


	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<!-- Your custom nav footer here -->
	
</footer>
	

</nav>

      
      <!-- Icon menu -->
  	  <a id="nav-menu">
  	  	<div id="menu"></div>
  	  </a>
  
        <!-- Header -->
        
          <header id="header">

	<!-- Your custom header here -->
	<span class="f-left">  
		<a href="/">
			<h1>
				<span>Youngwon Seo</span>
			</h1>
		</a>
	</span>

	<span id="nav-links" class="absolute right bottom">
		<!-- Nav pages -->
		
			
		
			
		
			
				<a href="/about/" title="About">About</a>
			
		
			
				<a href="/archive/" title="Archive">Archive</a>
			
		
			
		
			
		
			
		
			
				<a href="/ml/" title="Machine Learning">Machine Learning</a>
			
		
			
				<a href="/paper_review/" title="Paper Review">Paper Review</a>
			
		
			
				<a href="/projects/" title="Projects">Projects</a>
			
		
			
		
			
				<a href="/study/" title="Study">Study</a>
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
			
		
		
		<!-- Nav links -->
		<!-- <a href="/">Blog</a> -->

	</span>
	
	<!-- Google Analytics -->
  <script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
		
		ga('create', 'UA-78263144-1', 'auto');
		ga('send', 'pageview');
		</script>
		<!-- End Google Analytics -->
</header>


        
  
      <!-- Main content -->
  	  <div id="container">
  		  
  		<main>
  
  			<article id="post-page">
  <div class="parent f-right">
    <!-- Twitter -->
    <a href="https://twitter.com/share?url=https://thereviewindex.com/blog/2019/11/15/06-01-keras/&amp;text=텍스트 데이터 처리하기 - 순환 신경망&amp;hashtags=thereviewindex" target="_blank">
      <span class="social-icon">  
        <svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"></path></svg>
      </span>
    </a>

    &nbsp;&nbsp;&nbsp;
    <!-- Facebook -->
    <a href="http://www.facebook.com/sharer.php?u=https://thereviewindex.com/blog/2019/11/15/06-01-keras/" target="_blank">
      <span class="social-icon">
        <svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"></path></svg>
      </span> 
    </a>
  </div>
  
  <h2>텍스트 데이터 처리하기 - 순환 신경망</h2>		
	<div class="by-line parent">
    <time datetime="2019-11-15T00:00:00+09:00"> <i>15 Nov 2019</i> </time>
    &nbsp;&nbsp; &middot; &nbsp;&nbsp;
    <span><i></i> </span>
  </div>
	<div class="content">

		<p>이번장에서는 텍스트(단어의 시퀀스 또는 문자의 시퀀스), 시계열 또는 시퀀스 데이터를 처리하는 딥러닝 모델을 살펴봅니다. 기본적으로 시퀀스 데이터 처리를 위한 딥러닝 모델은 순환 신경망(recurrent neural network)과 1D 컨브넷(1D convnet)입니다.</p>

<p>먼저 가장 대표적인 시퀀스 데이터인 텍스트(문장 및 단어)를 다뤄보겠습니다.</p>

<h2 id="1-텍스트-데이터-다루기">1. 텍스트 데이터 다루기</h2>
<p>텍스트 패턴인식</p>

<p>다음은</p>
<ul>
  <li>텍스트를 단어로 나누고 각 단어를 하나의 백터로 변환</li>
  <li>텍스트를 문자로 나누고 각 문자를 하나의 백터로 변환</li>
  <li>텍스트에서 단어나 문자의 n-그램을 추출하여 각 n-그램을 하나의 백터로 변환</li>
</ul>

<p>텍스트를 나누는 이런 단위(단어, 문자, n-그램)를 토큰(나누는 작업은 토큰화, tokenization)이라 합니다. 텍스트 백터화 과정은 이런 토큰화를 통해 생성되는 각 토큰을 백터화하는 것입니다. 여기서는 다음과 같은 토큰-&gt;백터화 방법을 소개합니다.</p>

<ul>
  <li>원-핫 인코딩(one-hot encoding)</li>
  <li>토큰 임베딩(token embedding)</li>
</ul>

<h3 id="11-원-핫-인코딩">1.1 원-핫 인코딩</h3>
<p>먼저 살펴볼 원-핫 인코딩 방식은 3장의 영화리뷰(IMDB)나 뉴스기사(로이터)를 분류하기 위해 사용된 방법입니다. 이 방법은 원하는 토큰 단위에 따라 고유한 인덱스를 부여하고 각 토큰에 해당 인덱스만 1로 구성된 백터로 변환합니다.</p>

<p>코드를 살펴봅시다.</p>

<h4 id="단어단위-원-핫-인코딩">단어단위 원-핫 인코딩</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="s">'The cat sat on the mat.'</span><span class="p">,</span> <span class="s">'The dog ate my homework.'</span><span class="p">]</span>

<span class="n">token_index</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sample</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">token_index</span><span class="p">:</span>
      <span class="n">token_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1">#단어마다 인덱스 할당, 인덱스는 1씩 증가
</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span> <span class="n">max_length</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">token_index</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">split</span><span class="p">()))[:</span><span class="n">max_length</span><span class="p">]:</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">token_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
</code></pre></div></div>
<p>예제를 위해 간단한 두개의 문장이 존재합니다. 먼저 token_index는 각 단어가 나타내는 인덱스를 표현합니다. 다음과 같이 입력되어 있습니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="s">'The'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
 <span class="s">'ate'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
 <span class="s">'cat'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
 <span class="s">'dog'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
 <span class="s">'homework.'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
 <span class="s">'mat.'</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
 <span class="s">'my'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span>
 <span class="s">'on'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
 <span class="s">'sat'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
 <span class="s">'the'</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
</code></pre></div></div>
<p>dict의 키로 단어가 입력되고 각 단어별 인덱스가 할당된 형태입니다.</p>

<p>max_length는 각 문장이 원-핫 백터로 인코딩될때 10개의 단어를 표현하는 백터로 변환한다는 의미입니다. 즉 첫번째 문장은 단어가 6개로 이루어 져 있으므로 각 단어가 하나의 백터로 표현되면 다음과 같이 나머지 비어있는 4개의 영백터가 추가됩니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>    <span class="c1">#The의 1을 의미
</span>  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>   <span class="c1">#cat의 2를 의미
</span>  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>   <span class="c1">#단어가 6개 밖에없어 전부 0으로 채워진 영백터로 max_length까지 표현
</span>  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> 
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]]</span>
</code></pre></div></div>
<p>그리고 만약 단어가 15개로 이루어져 있는 문장을 위와 같이 인코딩한다면 뒤에 5개의 단어는 짤리게 됩니다.</p>

<p>아래의 코드는 단어가 아닌 문자기준으로 표현하는 원-핫 인코딩 방식의 코드입니다.</p>

<h4 id="문자단위-원-핫-인코딩">문자단위 원-핫 인코딩</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">string</span>

<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="s">'The cat sat on the mat.'</span><span class="p">,</span> <span class="s">'The dog ate my homework.'</span><span class="p">]</span>
<span class="n">characters</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">printable</span>
<span class="n">token_index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">characters</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">characters</span><span class="p">))))</span>

<span class="n">max_length</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">max_length</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">token_index</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">character</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">token_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">character</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
</code></pre></div></div>
<p>여기서는 총 50개의 문자를 가지는 문자열을 포함했습니다. token_index의 일부만 살펴보겠씁니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">:</span> <span class="mi">96</span><span class="p">,</span>
 <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">:</span> <span class="mi">97</span><span class="p">,</span>
 <span class="s">'</span><span class="se">\x0b</span><span class="s">'</span><span class="p">:</span> <span class="mi">99</span><span class="p">,</span>
 <span class="s">'</span><span class="se">\r</span><span class="s">'</span><span class="p">:</span> <span class="mi">98</span><span class="p">,</span>
 <span class="s">' '</span><span class="p">:</span> <span class="mi">95</span><span class="p">,</span>
 <span class="s">'!'</span><span class="p">:</span> <span class="mi">63</span><span class="p">,</span>
 <span class="s">'"'</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
 <span class="s">'#'</span><span class="p">:</span> <span class="mi">65</span><span class="p">,</span>
 <span class="s">'$'</span><span class="p">:</span> <span class="mi">66</span><span class="p">,</span>
 <span class="o">...</span>
 <span class="s">'5'</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
 <span class="s">'6'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
 <span class="s">'7'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
 <span class="s">'8'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span>
 <span class="s">'9'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
 <span class="o">...</span>
  <span class="s">'S'</span><span class="p">:</span> <span class="mi">55</span><span class="p">,</span>
 <span class="s">'T'</span><span class="p">:</span> <span class="mi">56</span><span class="p">,</span>
 <span class="s">'U'</span><span class="p">:</span> <span class="mi">57</span><span class="p">,</span>
 <span class="s">'V'</span><span class="p">:</span> <span class="mi">58</span><span class="p">,</span>
 <span class="s">'W'</span><span class="p">:</span> <span class="mi">59</span><span class="p">,</span>
 <span class="s">'X'</span><span class="p">:</span> <span class="mi">60</span><span class="p">,</span>
 <span class="s">'Y'</span><span class="p">:</span> <span class="mi">61</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>
<p>영어도 있고 숫자도 있고 특수문자나 이스케이프문자(\n 등)도 포함되어 있습니다.</p>

<h4 id="케라스를-사용한-원-핫-인코딩">케라스를 사용한 원-핫 인코딩</h4>

<p>다음은 케라스 API를 사용한 원-핫 인코딩의 코드입니다. 설명은 주석으로 대체합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>

<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="s">'The cat sat on the mat.'</span><span class="p">,</span> <span class="s">'The dog ate my homework.'</span><span class="p">]</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span> <span class="c1">#가장 빈도가 높은 단어 1000개를 사용하도록 합니다.
</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="c1">#단어인덱스를 만듭니다. tokenizer에 만들어집니다.
</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>  <span class="c1">#문장을 단어인덱스의 인덱스로 표현합니다. 첫 번째 문장의 경우 [1, 2, 3, 4, 1, 5] 이렇게 표현, 각 인덱스는 각 단어
</span>
<span class="n">one_hot_results</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_matrix</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'binary'</span><span class="p">)</span> <span class="c1">#인덱스의 시퀀스로 표현된 문장을 원핫으로 인코딩 합니다.
</span>
<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span> <span class="c1">#단어 인덱스가 word_index에 포함되어 있습니다. 이전 코드의 token_index처럼 각 단어가 키이고 고유인덱스가 값으로 구성된 dict입니다.
</span><span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="si">%</span><span class="s">s개의 고유한 토큰을 찾았습니다.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="원-핫-해싱">원-핫 해싱</h3>
<p>원-핫 인코딩의 변종인 원-핫 해싱(one-hot hashing)이 존재합니다. 이 방식은 고유한 토큰수(단어 또는 문자)가 너무 커서 토큰에 대한 백터가 너무 클때 사용됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="s">'The cat sat on the mat.'</span><span class="p">,</span> <span class="s">'The dog ate my homework.'</span><span class="p">]</span>

<span class="n">dimensionality</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1">#최대 1000개의 단어를 표현하는 백터를 의미
</span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">dimensionality</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">split</span><span class="p">()))[:</span><span class="n">max_length</span><span class="p">]:</span> <span class="c1">#문장에서 최대 10개의 토큰을 표현
</span>    <span class="n">index</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="nb">hash</span><span class="p">(</span><span class="n">word</span><span class="p">))</span> <span class="o">%</span> <span class="n">dimensionality</span> <span class="c1"># token_index를 미리 생성하는것이 아니라 해시를 사용해서 index를 부여
</span>    <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">index</span><span class="p">]</span>  <span class="o">=</span> <span class="mf">1.</span>
</code></pre></div></div>
<p>최대 1000개 이지만 그 이상의 단어가 표현될 경우 충돌이 발생합니다.</p>

<h3 id="12-단어-임베딩-사용하기">1.2 단어 임베딩 사용하기</h3>
<p>단어를 백터로 표현하는 또 다른 방법은 단어 임베딩입니다. 원핫인코딩을 생각해 봅시다. 단어가 1000가지 라면 하나의 토큰을 표현하는데 1000개의 원소를 가진 백터가 필요합니다(고차원). 거기에다 단 하나의 인덱스에만 1이 입력되어 있고 나저미는 0이 채워집니다(희소). 하나의 토큰을 표현하기 위해 많은 메모리를 사용하죠. 원핫인코딩과 다르게 <strong>단어 임베딩</strong>은 고정된 백터를 사용하고 1이나 0이 아닌 실수의 집합으로 단어를 표현하는 방식입니다.</p>

<h4 id="원핫-인코딩">원핫 인코딩</h4>
<ul>
  <li>고차원</li>
  <li>희소(sparse)</li>
</ul>

<h4 id="단어임베딩">단어임베딩</h4>
<ul>
  <li>실수형 백터 사용</li>
  <li>저차원</li>
  <li>밀집(density)</li>
</ul>

<p>단어 임베딩을 만드는 2가지 방법</p>
<ul>
  <li>관심대상인 문제와 함께 단어 임베딩을 학습, 즉 문제에 표함되어 있는 단어들을 학습</li>
  <li>미리 계산된 단어 임베딩(pretrained word embedding)을 사용, 즉 일반화 되어 있는 단어집으로 학습된 단어임베딩을 사용</li>
</ul>

<p>각각의 방법에 대해 알아보겠습니다.</p>

<h4 id="케라스의-embedding층을-사용하여-단어-임베딩-학습하기">케라스의 Embedding층을 사용하여 단어 임베딩 학습하기</h4>

<p>케라스의 Embedding층을 사용하면 다음과 같이 쉽게 단어임베딩을 학습할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span>
<span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="c1">#가능한 토큰개수 1000, 임베딩 차원(토큰을 표현하는 백터의 크기) 64
</span></code></pre></div></div>

<p>Embedding층은 학습할 단어의 수(samples), 하나의 단어를 표현하는 차원수(sequence_length)를 파라미터로 받습니다. 위 코드는 총 1000개의 단어가 존재하고 하나의 단어는 64개의 실수로 표현한다는 의미입니다. 입력 데이터를 받으면 Embedding층은 ()</p>

<h4 id="사전-훈련된-단어-임베딩-사용하기">사전 훈련된 단어 임베딩 사용하기</h4>

<h3 id="13-imdb데이터-셋으로-단어-임베딩-학습하기">1.3 IMDB데이터 셋으로 단어 임베딩 학습하기</h3>
<p>여기서는 이전에 영화분류를 위해 사용했던 데이터셋(IMDB)를 가지고 단어임베딩을 학습해 보겠습니다.</p>

<h4 id="원본-imdb-텍스트-내려받기">원본 IMDB 텍스트 내려받기</h4>
<p>IMDB의 원본데이터를 http://mng.bz/0tIo 에서 다운받습니다. 본 파일은 train과 test폴도로 나누어져 있고 다시 neg와 pos 폴더로 긍/부정 데이터가 나누어져 있습니다. 여기서는 훈련데이터셋의 neg와 pos를 텍스트와 레이블의 리스트로 불러옵니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>

<span class="n">imdb_dir</span> <span class="o">=</span> <span class="s">'./datasets/aclImdb'</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">imdb_dir</span><span class="p">,</span> <span class="s">'train'</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">label_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'neg'</span><span class="p">,</span> <span class="s">'pos'</span><span class="p">]:</span>
    <span class="n">dir_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">label_type</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">dir_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">fname</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span> <span class="o">==</span> <span class="s">'.txt'</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">fname</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf8'</span><span class="p">)</span>
            <span class="n">texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
            <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">label_type</span> <span class="o">==</span> <span class="s">'neg'</span><span class="p">:</span>
                <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#부정이면 0
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#긍정이면 1
</span></code></pre></div></div>
<p>text와 labels은 인덱스로 매칭이 됩니다.</p>

<h4 id="데이터-토큰화">데이터 토큰화</h4>
<p>여기서는 사전훈련된 단어 임베딩을 같이 사용합니다(<strong>문제에 특화된 데이터 셋이 충분히 있다면 그 데이터 셋으로 단어임베딩을 만드는 것이 문제 해결에 있어서 훨씬 성능이 좋습니다.</strong>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">maxlen</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># 텍스트 1개당 최대 100개의 단어를 사용
</span><span class="n">training_samples</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1"># 200개의 훈련 데이터 사용
</span><span class="n">validation_samples</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c1"># 검증은 1만개의 데이터 사용
</span><span class="n">max_words</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c1"># 데이터셋에서 가장 빈도가 높은 1만개의 단어사용
</span>
<span class="c1"># 데이터 셋에서 1만개의 토큰(단어)를 추출
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_words</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span> <span class="c1">#텍스트를 토큰인덱스에 대한 백터로 변환
</span>
<span class="c1"># 단어:인덱스 에 대한 딕셔너리
</span><span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>
<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="si">%</span><span class="s">s개의 고유한 토큰을 찾았습니다.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">))</span>

<span class="c1">#텍스트가 포함한 단어의 수가 maxlen보다 0으로 채움, 길면maxlen이후는 버림
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'데이터 텐서의 크기:'</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'레이블 텐서의 크기:'</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">#데이터 개수에 대한 넘파이 배열(0~24999)을 랜덤하게 썩도 추출
</span><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

<span class="c1">#200개의 훈련셋, 10000개의 검증셋
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="n">training_samples</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:</span><span class="n">training_samples</span><span class="p">]</span>
<span class="n">x_val</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">training_samples</span><span class="p">:</span> <span class="n">training_samples</span> <span class="o">+</span> <span class="n">validation_samples</span><span class="p">]</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">training_samples</span><span class="p">:</span> <span class="n">training_samples</span> <span class="o">+</span> <span class="n">validation_samples</span><span class="p">]</span>
</code></pre></div></div>

<h4 id="glove-단어-임베딩-내려받고-임베딩-불러오기">GloVe 단어 임베딩 내려받고 임베딩 불러오기</h4>
<p>https://nlp.stanford.edu/projects/glove에서 2014년에 영문 위키피디아를 이용해 사전훈련된 임베딩을 다운받습니다. 40만개의 단어와 100차원 임베딩 벡터를 포함하고 있습니다. 즉 40만 개의 토큰이 존재하고 하나의 토큰은 100개의 실수로 표현된 데이터 입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">glove_dir</span> <span class="o">=</span> <span class="s">'./datasets'</span>

<span class="n">embeddings_index</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">glove_dir</span><span class="p">,</span> <span class="s">'glove.6B.100d.txt'</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf8'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="c1">#단어 실수1 실수2 .. 실수100
</span>    <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#첫번째는 단어
</span>    <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">)</span> <span class="c1">#나머지는 100개의 실수
</span>    <span class="n">embeddings_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="si">%</span><span class="s">s개의 단어 백터를 찾았습니다.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">))</span>
</code></pre></div></div>

<p>영화리뷰데이터셋에서 뽑아온 1만개의 단어에 대한 임베딩을 GloVe 임베딩에서 가져옵니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">max_words</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">))</span> <span class="c1">#(10000, 100) 의 임베딩 행렬
</span><span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span> <span class="c1">#영화리뷰에서 최대 10000개의 데이터 추출
</span>    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_words</span><span class="p">:</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embeddings_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="c1">#사전 학습된 Glove 임베딩에 해당 단어가 존재하면 가져오기
</span>        <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
</code></pre></div></div>

<h4 id="모델-정의">모델 정의</h4>

<p>이전 분류모델과 같은 모델입니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_words</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">maxlen</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="모델에-glove-임베딩-로드">모델에 GloVe 임베딩 로드</h4>
<p>Embedding 층은 하나의 가중치 행렬을 가집니다. 이 행렬은 입력인덱스에 대해 임베딩 결과를 출력하는 행렬입니다. 사전훈된된 결과를 주입하고 학습이 안되도록 파라미터를 설정합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_weights</span><span class="p">([</span><span class="n">embedding_matrix</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div>

<h4 id="모델-훈련과-평가">모델 훈련과 평가</h4>

<p>모델을 평가하고 결과를 시각화해 봅시다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span>
             <span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span>
             <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                   <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                   <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span><span class="n">y_val</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s">'pre_trained_glove_model.h5'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'acc'</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/public/keras/embedding_1.png" alt="" /></p>

<p><img src="/public/keras/embedding_2.png" alt="" /></p>

<p>아주 빠르게 과적합 되는것을 확인 할 수 있습니다. 이번에는 테스트 데이터에 모델을 적용해 보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">imdb_dir</span><span class="p">,</span> <span class="s">'test'</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">label_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'neg'</span><span class="p">,</span> <span class="s">'pos'</span><span class="p">]:</span>
    <span class="n">dir_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">label_type</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">dir_name</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">fname</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span> <span class="o">==</span> <span class="s">'.txt'</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">fname</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf8'</span><span class="p">)</span>
            <span class="n">texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
            <span class="n">f</span><span class="o">.</span><span class="n">close</span>
            <span class="k">if</span> <span class="n">label_type</span> <span class="o">==</span> <span class="s">'neg'</span><span class="p">:</span>
                <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="c1">#모델 불러와서 평가
</span><span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">'pre_trained_glove_model.h5'</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaludate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[0.7864487828063965, 0.57648]
0.57 의 정확도가 측정되었습니다.</p>
</blockquote>


		
  </div>

  <div class="parent f-right">
    <!-- Twitter -->
    <a href="https://twitter.com/share?url=https://thereviewindex.com/blog/2019/11/15/06-01-keras/&amp;text=텍스트 데이터 처리하기 - 순환 신경망&amp;hashtags=thereviewindex" target="_blank">
      <span class="social-icon">  
        <svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"></path></svg>
      </span>
    </a>

    &nbsp;&nbsp;&nbsp;
    <!-- Facebook -->
    <a href="http://www.facebook.com/sharer.php?u=https://thereviewindex.com/blog/2019/11/15/06-01-keras/" target="_blank">
      <span class="social-icon">
        <svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29"><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"></path></svg>
      </span> 
    </a>
  </div>

  <!-- Facebook Comments HTML -->
  <div class="m-t-6e">
    <div class="fb-comments" data-href="http://youngwonseo.github.io/2019/11/15/06-01-keras/" data-width="800" data-numposts="5"></div>
  </div>

</article>


  
  	  </main>
  		
  		  <!-- Pagination links -->
        
  
  	  </div>
  	    
  	    <!-- Footer -->
        <div class="m-t-6e">
          <footer><span>&#169;2017 - TheReviewIndex</span></footer>

        </div>
  
  	    <!-- Script -->
        <script src="/js/main.js"></script>	

  
    </div>
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118401942-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-118401942-1');
</script>
  </body>
</html>
