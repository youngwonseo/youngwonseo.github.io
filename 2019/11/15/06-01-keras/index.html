<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta https-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width initial-scale=1">

  <title>텍스트 데이터 처리하기 - 순환 신경망</title>
  <meta name="description" content="이번장에서는 텍스트(단어의 시퀀스 또는 문자의 시퀀스), 시계열 또는 시퀀스 데이터를 처리하는 딥러닝 모델을 살펴봅니다. 기본적으로 시퀀스 데이터 처리를 위한 딥러닝 모델은 순환 신경망(recurrent neural network)과 1D 컨브넷(1D convnet)입니다.">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://youngwonseo.github.io/2019/11/15/06-01-keras/">
  <link rel="alternate" type="application/atom+xml" title="YoungWon" href="https://youngwonseo.github.io/feed.xml" />
</head>

  <body>
    

<div class="header-container" id="header-container">

<!-- Site navigation -->
  <nav class="site-nav">
    <div class="trigger">
      
        
        <a class="page-link" href="/about/">About</a>
        
      
        
        <a class="page-link" href="/archive/">Archive</a>
        
      
        
      
        
      
        
        <a class="page-link" href="/projects/">Projects</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
      <a class="page-link" href="/feed.xml">RSS</a>
      <!-- <a class="twitter page-link" href="httpss://twitter.com/fffabs"><span class="icon-twitter"></span></a>
      <a class="facebook page-link" href="httpss://www.facebook.com/fffabs"><span class="icon-facebook"></span></a> -->
    </div>
  </nav>

  <!-- The title of the site -->
  <header class="site-header">
    <!-- <a href="/">
      <div class="trigger">
        <img src="/assets/images/avatar.png" />
      </div>
    </a> -->
    <a class="site-title" href="/">YoungWon</a> -->
  </header>

</div>

      <div class="wrapper">
        <div class="page-content">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">텍스트 데이터 처리하기 - 순환 신경망</h1>
    <p class="post-meta">November 15, 2019 • Youngwon Seo</p>
  </header>

  <!-- Beginning Twitter sharing Large button -->
  <!-- <a class="twitter-share-button" href="httpss://twitter.com/share"
  data-related="twitterdev"
  data-size="large"
  data-count="horizontal">
  Tweet
  </a> -->
  <!-- End of Twitter sharing button -->

  <!-- Facebook -->
  

  <article class="post-content">
    <p>이번장에서는 텍스트(단어의 시퀀스 또는 문자의 시퀀스), 시계열 또는 시퀀스 데이터를 처리하는 딥러닝 모델을 살펴봅니다. 기본적으로 시퀀스 데이터 처리를 위한 딥러닝 모델은 순환 신경망(recurrent neural network)과 1D 컨브넷(1D convnet)입니다.</p>

<p>먼저 가장 대표적인 시퀀스 데이터인 텍스트(문장 및 단어)를 다뤄보겠습니다.</p>

<h2 id="1-텍스트-데이터-다루기">1. 텍스트 데이터 다루기</h2>
<p>텍스트 패턴인식</p>

<p>다음은</p>
<ul>
  <li>텍스트를 단어로 나누고 각 단어를 하나의 백터로 변환</li>
  <li>텍스트를 문자로 나누고 각 문자를 하나의 백터로 변환</li>
  <li>텍스트에서 단어나 문자의 n-그램을 추출하여 각 n-그램을 하나의 백터로 변환</li>
</ul>

<p>텍스트를 나누는 이런 단위(단어, 문자, n-그램)를 토큰(나누는 작업은 토큰화, tokenization)이라 합니다. 텍스트 백터화 과정은 이런 토큰화를 통해 생성되는 각 토큰을 백터화하는 것입니다. 여기서는 다음과 같은 토큰-&gt;백터화 방법을 소개합니다.</p>

<ul>
  <li>원-핫 인코딩(one-hot encoding)</li>
  <li>토큰 임베딩(token embedding)</li>
</ul>

<h3 id="11-원-핫-인코딩">1.1 원-핫 인코딩</h3>
<p>먼저 살펴볼 원-핫 인코딩 방식은 3장의 영화리뷰(IMDB)나 뉴스기사(로이터)를 분류하기 위해 사용된 방법입니다. 이 방법은 원하는 토큰 단위에 따라 고유한 인덱스를 부여하고 각 토큰에 해당 인덱스만 1로 구성된 백터로 변환합니다.</p>

<p>코드를 살펴봅시다.</p>

<h4 id="단어단위-원-핫-인코딩">단어단위 원-핫 인코딩</h4>
<pre><code class="language-python">import numpy as np

samples = ['The cat sat on the mat.', 'The dog ate my homework.']

token_index = {}
for sample in samples:
  for word in sample.split():
    if word not in token_index:
      token_index[word] = len(token_index) + 1 #단어마다 인덱스 할당, 인덱스는 1씩 증가

max_length = 10

results = np.zeros(shape=(len(sample), max_length, max(token_index.values()) + 1))

for i, sample in enumerate(samples):
  for j, word in list(enumerate(sample.split()))[:max_length]:
    index = token_index.get(word)
    results[i, j, index] = 1.
</code></pre>
<p>예제를 위해 간단한 두개의 문장이 존재합니다. 먼저 token_index는 각 단어가 나타내는 인덱스를 표현합니다. 다음과 같이 입력되어 있습니다.</p>
<pre><code class="language-python">{'The': 1,
 'ate': 8,
 'cat': 2,
 'dog': 7,
 'homework.': 10,
 'mat.': 6,
 'my': 9,
 'on': 4,
 'sat': 3,
 'the': 5}
</code></pre>
<p>dict의 키로 단어가 입력되고 각 단어별 인덱스가 할당된 형태입니다.</p>

<p>max_length는 각 문장이 원-핫 백터로 인코딩될때 10개의 단어를 표현하는 백터로 변환한다는 의미입니다. 즉 첫번째 문장은 단어가 6개로 이루어 져 있으므로 각 단어가 하나의 백터로 표현되면 다음과 같이 나머지 비어있는 4개의 영백터가 추가됩니다.</p>
<pre><code class="language-python">[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],    #The의 1을 의미
  [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],   #cat의 2를 의미
  [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
  [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
  [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
  [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
  [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],   #단어가 6개 밖에없어 전부 0으로 채워진 영백터로 max_length까지 표현
  [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 
  [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
  [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]
</code></pre>
<p>그리고 만약 단어가 15개로 이루어져 있는 문장을 위와 같이 인코딩한다면 뒤에 5개의 단어는 짤리게 됩니다.</p>

<p>아래의 코드는 단어가 아닌 문자기준으로 표현하는 원-핫 인코딩 방식의 코드입니다.</p>

<h4 id="문자단위-원-핫-인코딩">문자단위 원-핫 인코딩</h4>
<pre><code class="language-python">import string

samples = ['The cat sat on the mat.', 'The dog ate my homework.']
characters = string.printable
token_index = dict(zip(characters, range(1, len(characters))))

max_length = 50
results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))
for i, sample in enumerate(samples):
  for j, character in enumerate(sample):
    index = token_index.get(character)
    results[i, j, index] = 1.
</code></pre>
<p>여기서는 총 50개의 문자를 가지는 문자열을 포함했습니다. token_index의 일부만 살펴보겠씁니다.</p>
<pre><code class="language-python">{'\t': 96,
 '\n': 97,
 '\x0b': 99,
 '\r': 98,
 ' ': 95,
 '!': 63,
 '"': 64,
 '#': 65,
 '$': 66,
 ...
 '5': 6,
 '6': 7,
 '7': 8,
 '8': 9,
 '9': 10,
 ...
  'S': 55,
 'T': 56,
 'U': 57,
 'V': 58,
 'W': 59,
 'X': 60,
 'Y': 61,
}
</code></pre>
<p>영어도 있고 숫자도 있고 특수문자나 이스케이프문자(\n 등)도 포함되어 있습니다.</p>

<h4 id="케라스를-사용한-원-핫-인코딩">케라스를 사용한 원-핫 인코딩</h4>

<p>다음은 케라스 API를 사용한 원-핫 인코딩의 코드입니다. 설명은 주석으로 대체합니다.</p>

<pre><code class="language-python">from keras.preprocessing.text import Tokenizer

samples = ['The cat sat on the mat.', 'The dog ate my homework.']

tokenizer = Tokenizer(num_words=1000) #가장 빈도가 높은 단어 1000개를 사용하도록 합니다.
tokenizer.fit_on_texts(samples) #단어인덱스를 만듭니다. tokenizer에 만들어집니다.

sequences = tokenizer.texts_to_sequences(samples)  #문장을 단어인덱스의 인덱스로 표현합니다. 첫 번째 문장의 경우 [1, 2, 3, 4, 1, 5] 이렇게 표현, 각 인덱스는 각 단어

one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary') #인덱스의 시퀀스로 표현된 문장을 원핫으로 인코딩 합니다.

word_index = tokenizer.word_index #단어 인덱스가 word_index에 포함되어 있습니다. 이전 코드의 token_index처럼 각 단어가 키이고 고유인덱스가 값으로 구성된 dict입니다.
print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))
</code></pre>

<h3 id="원-핫-해싱">원-핫 해싱</h3>
<p>원-핫 인코딩의 변종인 원-핫 해싱(one-hot hashing)이 존재합니다. 이 방식은 고유한 토큰수(단어 또는 문자)가 너무 커서 토큰에 대한 백터가 너무 클때 사용됩니다.</p>

<pre><code class="language-python">samples = ['The cat sat on the mat.', 'The dog ate my homework.']

dimensionality = 1000 #최대 1000개의 단어를 표현하는 백터를 의미
max_length = 10

results = np.zeros((len(samples), max_length, dimensionality))
for i, sample in enumerate(samples):
  for j, word in list(enumerate(sample.split()))[:max_length]: #문장에서 최대 10개의 토큰을 표현
    index = abs(hash(word)) % dimensionality # token_index를 미리 생성하는것이 아니라 해시를 사용해서 index를 부여
    results[i, j, index]  = 1.
</code></pre>
<p>최대 1000개 이지만 그 이상의 단어가 표현될 경우 충돌이 발생합니다.</p>

<h3 id="12-단어-임베딩-사용하기">1.2 단어 임베딩 사용하기</h3>
<p>단어를 백터로 표현하는 또 다른 방법은 단어 임베딩입니다. 원핫인코딩을 생각해 봅시다. 단어가 1000가지 라면 하나의 토큰을 표현하는데 1000개의 원소를 가진 백터가 필요합니다(고차원). 거기에다 단 하나의 인덱스에만 1이 입력되어 있고 나저미는 0이 채워집니다(희소). 하나의 토큰을 표현하기 위해 많은 메모리를 사용하죠. 원핫인코딩과 다르게 <strong>단어 임베딩</strong>은 고정된 백터를 사용하고 1이나 0이 아닌 실수의 집합으로 단어를 표현하는 방식입니다.</p>

<h4 id="원핫-인코딩">원핫 인코딩</h4>
<ul>
  <li>고차원</li>
  <li>희소(sparse)</li>
</ul>

<h4 id="단어임베딩">단어임베딩</h4>
<ul>
  <li>실수형 백터 사용</li>
  <li>저차원</li>
  <li>밀집(density)</li>
</ul>

<p>단어 임베딩을 만드는 2가지 방법</p>
<ul>
  <li>관심대상인 문제와 함께 단어 임베딩을 학습, 즉 문제에 표함되어 있는 단어들을 학습</li>
  <li>미리 계산된 단어 임베딩(pretrained word embedding)을 사용, 즉 일반화 되어 있는 단어집으로 학습된 단어임베딩을 사용</li>
</ul>

<p>각각의 방법에 대해 알아보겠습니다.</p>

<h4 id="케라스의-embedding층을-사용하여-단어-임베딩-학습하기">케라스의 Embedding층을 사용하여 단어 임베딩 학습하기</h4>

<p>케라스의 Embedding층을 사용하면 다음과 같이 쉽게 단어임베딩을 학습할 수 있습니다.</p>

<pre><code class="language-python">from keras.layers import Embedding
embedding_layer = Embedding(1000, 64) #가능한 토큰개수 1000, 임베딩 차원(토큰을 표현하는 백터의 크기) 64
</code></pre>

<p>Embedding층은 학습할 단어의 수(samples), 하나의 단어를 표현하는 차원수(sequence_length)를 파라미터로 받습니다. 위 코드는 총 1000개의 단어가 존재하고 하나의 단어는 64개의 실수로 표현한다는 의미입니다. 입력 데이터를 받으면 Embedding층은 ()</p>

<h4 id="사전-훈련된-단어-임베딩-사용하기">사전 훈련된 단어 임베딩 사용하기</h4>

<h3 id="13-imdb데이터-셋으로-단어-임베딩-학습하기">1.3 IMDB데이터 셋으로 단어 임베딩 학습하기</h3>
<p>여기서는 이전에 영화분류를 위해 사용했던 데이터셋(IMDB)를 가지고 단어임베딩을 학습해 보겠습니다.</p>

<h4 id="원본-imdb-텍스트-내려받기">원본 IMDB 텍스트 내려받기</h4>
<p>IMDB의 원본데이터를 https://mng.bz/0tIo 에서 다운받습니다. 본 파일은 train과 test폴도로 나누어져 있고 다시 neg와 pos 폴더로 긍/부정 데이터가 나누어져 있습니다. 여기서는 훈련데이터셋의 neg와 pos를 텍스트와 레이블의 리스트로 불러옵니다.</p>
<pre><code class="language-python">import os

imdb_dir = './datasets/aclImdb'
train_dir = os.path.join(imdb_dir, 'train')

labels = []
texts = []

for label_type in ['neg', 'pos']:
    dir_name = os.path.join(train_dir, label_type)
    for fname in os.listdir(dir_name):
        if fname[-4:] == '.txt':
            f = open(os.path.join(dir_name, fname), encoding='utf8')
            texts.append(f.read())
            f.close()
            if label_type == 'neg':
                labels.append(0) #부정이면 0
            else:
                labels.append(1) #긍정이면 1
</code></pre>
<p>text와 labels은 인덱스로 매칭이 됩니다.</p>

<h4 id="데이터-토큰화">데이터 토큰화</h4>
<p>여기서는 사전훈련된 단어 임베딩을 같이 사용합니다(<strong>문제에 특화된 데이터 셋이 충분히 있다면 그 데이터 셋으로 단어임베딩을 만드는 것이 문제 해결에 있어서 훨씬 성능이 좋습니다.</strong>).</p>

<pre><code class="language-python">from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np

maxlen = 100 # 텍스트 1개당 최대 100개의 단어를 사용
training_samples = 200 # 200개의 훈련 데이터 사용
validation_samples = 10000 # 검증은 1만개의 데이터 사용
max_words = 10000 # 데이터셋에서 가장 빈도가 높은 1만개의 단어사용

# 데이터 셋에서 1만개의 토큰(단어)를 추출
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts) #텍스트를 토큰인덱스에 대한 백터로 변환

# 단어:인덱스 에 대한 딕셔너리
word_index = tokenizer.word_index
print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))

#텍스트가 포함한 단어의 수가 maxlen보다 0으로 채움, 길면maxlen이후는 버림
data = pad_sequences(sequences, maxlen=maxlen)
labels = np.asarray(labels)
print('데이터 텐서의 크기:', data.shape)
print('레이블 텐서의 크기:', labels.shape)

#데이터 개수에 대한 넘파이 배열(0~24999)을 랜덤하게 썩도 추출
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]

#200개의 훈련셋, 10000개의 검증셋
x_train = data[:training_samples]
y_train = labels[:training_samples]
x_val = data[training_samples: training_samples + validation_samples]
y_val = labels[training_samples: training_samples + validation_samples]
</code></pre>

<h4 id="glove-단어-임베딩-내려받고-임베딩-불러오기">GloVe 단어 임베딩 내려받고 임베딩 불러오기</h4>
<p>httpss://nlp.stanford.edu/projects/glove에서 2014년에 영문 위키피디아를 이용해 사전훈련된 임베딩을 다운받습니다. 40만개의 단어와 100차원 임베딩 벡터를 포함하고 있습니다. 즉 40만 개의 토큰이 존재하고 하나의 토큰은 100개의 실수로 표현된 데이터 입니다.</p>

<pre><code class="language-python">glove_dir = './datasets'

embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf8')
for line in f:
    values = line.split() #단어 실수1 실수2 .. 실수100
    word = values[0] #첫번째는 단어
    coefs = np.asarray(values[1:], dtype='float32') #나머지는 100개의 실수
    embeddings_index[word] = coefs
f.close()

print('%s개의 단어 백터를 찾았습니다.' % len(embeddings_index))
</code></pre>

<p>영화리뷰데이터셋에서 뽑아온 1만개의 단어에 대한 임베딩을 GloVe 임베딩에서 가져옵니다.</p>
<pre><code class="language-python">embedding_dim = 100

embedding_matrix = np.zeros((max_words, embedding_dim)) #(10000, 100) 의 임베딩 행렬
for word, i in word_index.items(): #영화리뷰에서 최대 10000개의 데이터 추출
    if i &lt; max_words:
        embedding_vector = embeddings_index.get(word) #사전 학습된 Glove 임베딩에 해당 단어가 존재하면 가져오기
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
</code></pre>

<h4 id="모델-정의">모델 정의</h4>

<p>이전 분류모델과 같은 모델입니다.</p>
<pre><code class="language-python">from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense

model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
</code></pre>

<h4 id="모델에-glove-임베딩-로드">모델에 GloVe 임베딩 로드</h4>
<p>Embedding 층은 하나의 가중치 행렬을 가집니다. 이 행렬은 입력인덱스에 대해 임베딩 결과를 출력하는 행렬입니다. 사전훈된된 결과를 주입하고 학습이 안되도록 파라미터를 설정합니다.</p>

<pre><code class="language-python">model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
</code></pre>

<h4 id="모델-훈련과-평가">모델 훈련과 평가</h4>

<p>모델을 평가하고 결과를 시각화해 봅시다.</p>
<pre><code class="language-python">model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['acc'])
history = model.fit(x_train, y_train,
                    epochs=10,
                   batch_size=32,
                   validation_data=(x_val,y_val))
model.save_weights('pre_trained_glove_model.h5')
</code></pre>

<pre><code class="language-python">import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation accuracy')
plt.legend()

plt.show()
</code></pre>

<p><img src="/public/keras/embedding_1.png" alt="" /></p>

<p><img src="/public/keras/embedding_2.png" alt="" /></p>

<p>아주 빠르게 과적합 되는것을 확인 할 수 있습니다. 이번에는 테스트 데이터에 모델을 적용해 보겠습니다.</p>

<pre><code class="language-python">test_dir = os.path.join(imdb_dir, 'test')

labels = []
texts = []

for label_type in ['neg', 'pos']:
    dir_name = os.path.join(test_dir, label_type)
    for fname in sorted(os.listdir(dir_name)):
        if fname[-4:] == '.txt':
            f = open(os.path.join(dir_name, fname), encoding='utf8')
            texts.append(f.read())
            f.close
            if label_type == 'neg':
                labels.append(0)
            else:
                labels.append(1)
                
sequences = tokenizer.texts_to_sequences(texts)
x_test = pad_sequences(sequences, maxlen=maxlen)
y_test = np.asarray(labels)

#모델 불러와서 평가
model.load_weights('pre_trained_glove_model.h5')
model.evaludate(x_test, y_test)
</code></pre>
<blockquote>
  <p>[0.7864487828063965, 0.57648]
0.57 의 정확도가 측정되었습니다.</p>
</blockquote>


  </article>

  <hr>

  
    <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: httpss://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'httpss://youngwonseo.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="httpss://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                            
  
  <!-- <div class="question">
    <h2>Questions?</h2>
    <p>Have a question regarding the post above? <br />Or any of my designs?</p>
    <a class="twitter-follow-button"
    href="httpss://twitter.com/fffabs"
    data-show-count="true"
    data-size="large">
    Follow @fffabs
    </a>
    <script type="text/javascript">
    window.twttr = (function (d, s, id) {
      var t, js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return;
      js = d.createElement(s); js.id = id;
      js.src= "httpss://platform.twitter.com/widgets.js";
      fjs.parentNode.insertBefore(js, fjs);
      return window.twttr || (t = { _e: [], ready: function (f) { t._e.push(f) } });
    }(document, "script", "twitter-wjs"));
    </script>
  </div> -->

  <div class="related">
    <h2>Related</h2>
    
      <li><a href="/2020/11/11/detecting-aortic-stenosis-using-electrocardiography/" title="Detecting Aortic Stenosis Using Electrocardiography">Detecting Aortic Stenosis Using Electrocardiography
       &nbsp; <span class="post-meta">November 11, 2020</span></a>
    
      <li><a href="/2020/10/09/image-resize/" title="파이썬 이미지 리사이즈">파이썬 이미지 리사이즈
       &nbsp; <span class="post-meta">October 09, 2020</span></a>
    
      <li><a href="/2020/09/22/class-activation-map/" title="Learning Deep Features for Discriminative Localization">Learning Deep Features for Discriminative Localization
       &nbsp; <span class="post-meta">September 22, 2020</span></a>
    
  </div>

</div>

        </div>
        <footer class="site-footer">
<p class="small">YoungwonSeo &copy 2020
</p>
</footer>

    </div>

    <script src="//cdn.jsdelivr.net/headroomjs/0.5.0/headroom.min.js"></script>
    <script type="text/javascript">
      var el = document.querySelector(".header-container");
      var headroom  = new Headroom(el, {
        "offset": 205,
        "tolerance": 5
      });
      headroom.init();
    </script>


    <!-- Twitter Shizzle -->
    <!-- <script type="text/javascript">
    window.twttr = (function (d, s, id) {
      var t, js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return;
      js = d.createElement(s); js.id = id;
      js.src= "httpss://platform.twitter.com/widgets.js";
      fjs.parentNode.insertBefore(js, fjs);
      return window.twttr || (t = { _e: [], ready: function (f) { t._e.push(f) } });
    }(document, "script", "twitter-wjs"));
    </script> -->
    	<!-- Google Analytics -->
  <script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','httpss://www.google-analytics.com/analytics.js','ga');
		
		ga('create', 'UA-78263144-1', 'auto');
		ga('send', 'pageview');
		</script>
		<!-- End Google Analytics -->
</header>


  </body>
</html>
