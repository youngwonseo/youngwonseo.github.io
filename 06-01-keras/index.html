<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width initial-scale=1">

  <title>텍스트 데이터 처리하기 - 순환 신경망</title>
  <meta name="description" content="이번장에서는 텍스트(단어의 시퀀스 또는 문자의 시퀀스), 시계열 또는 시퀀스 데이터를 처리하는 딥러닝 모델을 살펴봅니다. 기본적으로 시퀀스 데이터 처리를 위한 딥러닝 모델은 순환 신경망(recurrent neural network)과 1D 컨브넷(1D convnet)입니다.">
  
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://youngwonseo.github.io/06-01-keras/">
  <link rel="alternate" type="application/atom+xml" title="서영원 블로그" href="https://youngwonseo.github.io/feed.xml" />  
  <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/moonspam/NanumBarunGothic@latest/nanumbarungothicsubset.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
</head>

  <body>
    

<div class="header-container" id="header-container">

<!-- Site navigation -->
  <nav class="site-nav">
    <div class="trigger">
      
        
        <a class="page-link" href="/about/">About</a>
        
      
        
        <a class="page-link" href="/archive/">Archive</a>
        
      
        
      
        
      
        
      
        
        <a class="page-link" href="/category/">Paper</a>
        
      
        
        <a class="page-link" href="/projects/">Projects</a>
        
      
        
        <a class="page-link" href="/study/">Study</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
      <a class="page-link" href="/feed.xml">RSS</a>
    </div>
  </nav>

  <!-- The title of the site -->
  <header class="site-header">
    <!-- <a href="/">
      <div class="avatar">
        <img src="/assets/images/avatar.png" />
      </div>
    </a> -->
    <a class="site-title" href="/">서영원 블로그</a>
  </header>

</div>

      <div class="wrapper">
        <div class="page-content">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">텍스트 데이터 처리하기 - 순환 신경망</h1>
    <p class="post-meta">2019-11-15 / Youngwon Seo</p>
  </header>



  <article class="post-content">
    <p>이번장에서는 텍스트(단어의 시퀀스 또는 문자의 시퀀스), 시계열 또는 시퀀스 데이터를 처리하는 딥러닝 모델을 살펴봅니다. 기본적으로 시퀀스 데이터 처리를 위한 딥러닝 모델은 순환 신경망(recurrent neural network)과 1D 컨브넷(1D convnet)입니다.</p>

<p>먼저 가장 대표적인 시퀀스 데이터인 텍스트(문장 및 단어)를 다뤄보겠습니다.</p>

<h2 id="1-텍스트-데이터-다루기">1. 텍스트 데이터 다루기</h2>
<p>텍스트 패턴인식</p>

<p>다음은</p>
<ul>
  <li>텍스트를 단어로 나누고 각 단어를 하나의 백터로 변환</li>
  <li>텍스트를 문자로 나누고 각 문자를 하나의 백터로 변환</li>
  <li>텍스트에서 단어나 문자의 n-그램을 추출하여 각 n-그램을 하나의 백터로 변환</li>
</ul>

<p>텍스트를 나누는 이런 단위(단어, 문자, n-그램)를 토큰(나누는 작업은 토큰화, tokenization)이라 합니다. 텍스트 백터화 과정은 이런 토큰화를 통해 생성되는 각 토큰을 백터화하는 것입니다. 여기서는 다음과 같은 토큰-&gt;백터화 방법을 소개합니다.</p>

<ul>
  <li>원-핫 인코딩(one-hot encoding)</li>
  <li>토큰 임베딩(token embedding)</li>
</ul>

<h3 id="11-원-핫-인코딩">1.1 원-핫 인코딩</h3>
<p>먼저 살펴볼 원-핫 인코딩 방식은 3장의 영화리뷰(IMDB)나 뉴스기사(로이터)를 분류하기 위해 사용된 방법입니다. 이 방법은 원하는 토큰 단위에 따라 고유한 인덱스를 부여하고 각 토큰에 해당 인덱스만 1로 구성된 백터로 변환합니다.</p>

<p>코드를 살펴봅시다.</p>

<h4 id="단어단위-원-핫-인코딩">단어단위 원-핫 인코딩</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="s">'The cat sat on the mat.'</span><span class="p">,</span> <span class="s">'The dog ate my homework.'</span><span class="p">]</span>

<span class="n">token_index</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">.</span><span class="n">split</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">token_index</span><span class="p">:</span>
      <span class="n">token_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1">#단어마다 인덱스 할당, 인덱스는 1씩 증가
</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span> <span class="n">max_length</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">token_index</span><span class="p">.</span><span class="n">values</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">sample</span><span class="p">.</span><span class="n">split</span><span class="p">()))[:</span><span class="n">max_length</span><span class="p">]:</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">token_index</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
</code></pre></div></div>
<p>예제를 위해 간단한 두개의 문장이 존재합니다. 먼저 token_index는 각 단어가 나타내는 인덱스를 표현합니다. 다음과 같이 입력되어 있습니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="s">'The'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
 <span class="s">'ate'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
 <span class="s">'cat'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
 <span class="s">'dog'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
 <span class="s">'homework.'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
 <span class="s">'mat.'</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
 <span class="s">'my'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span>
 <span class="s">'on'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
 <span class="s">'sat'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
 <span class="s">'the'</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
</code></pre></div></div>
<p>dict의 키로 단어가 입력되고 각 단어별 인덱스가 할당된 형태입니다.</p>

<p>max_length는 각 문장이 원-핫 백터로 인코딩될때 10개의 단어를 표현하는 백터로 변환한다는 의미입니다. 즉 첫번째 문장은 단어가 6개로 이루어 져 있으므로 각 단어가 하나의 백터로 표현되면 다음과 같이 나머지 비어있는 4개의 영백터가 추가됩니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>    <span class="c1">#The의 1을 의미
</span>  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>   <span class="c1">#cat의 2를 의미
</span>  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>   <span class="c1">#단어가 6개 밖에없어 전부 0으로 채워진 영백터로 max_length까지 표현
</span>  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> 
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
  <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]]</span>
</code></pre></div></div>
<p>그리고 만약 단어가 15개로 이루어져 있는 문장을 위와 같이 인코딩한다면 뒤에 5개의 단어는 짤리게 됩니다.</p>

<p>아래의 코드는 단어가 아닌 문자기준으로 표현하는 원-핫 인코딩 방식의 코드입니다.</p>

<h4 id="문자단위-원-핫-인코딩">문자단위 원-핫 인코딩</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">string</span>

<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="s">'The cat sat on the mat.'</span><span class="p">,</span> <span class="s">'The dog ate my homework.'</span><span class="p">]</span>
<span class="n">characters</span> <span class="o">=</span> <span class="n">string</span><span class="p">.</span><span class="n">printable</span>
<span class="n">token_index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">characters</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">characters</span><span class="p">))))</span>

<span class="n">max_length</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">max_length</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">token_index</span><span class="p">.</span><span class="n">values</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">character</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample</span><span class="p">):</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">token_index</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">character</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
</code></pre></div></div>
<p>여기서는 총 50개의 문자를 가지는 문자열을 포함했습니다. token_index의 일부만 살펴보겠씁니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">:</span> <span class="mi">96</span><span class="p">,</span>
 <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">:</span> <span class="mi">97</span><span class="p">,</span>
 <span class="s">'</span><span class="se">\x0b</span><span class="s">'</span><span class="p">:</span> <span class="mi">99</span><span class="p">,</span>
 <span class="s">'</span><span class="se">\r</span><span class="s">'</span><span class="p">:</span> <span class="mi">98</span><span class="p">,</span>
 <span class="s">' '</span><span class="p">:</span> <span class="mi">95</span><span class="p">,</span>
 <span class="s">'!'</span><span class="p">:</span> <span class="mi">63</span><span class="p">,</span>
 <span class="s">'"'</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
 <span class="s">'#'</span><span class="p">:</span> <span class="mi">65</span><span class="p">,</span>
 <span class="s">'$'</span><span class="p">:</span> <span class="mi">66</span><span class="p">,</span>
 <span class="p">...</span>
 <span class="s">'5'</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
 <span class="s">'6'</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
 <span class="s">'7'</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
 <span class="s">'8'</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span>
 <span class="s">'9'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
 <span class="p">...</span>
  <span class="s">'S'</span><span class="p">:</span> <span class="mi">55</span><span class="p">,</span>
 <span class="s">'T'</span><span class="p">:</span> <span class="mi">56</span><span class="p">,</span>
 <span class="s">'U'</span><span class="p">:</span> <span class="mi">57</span><span class="p">,</span>
 <span class="s">'V'</span><span class="p">:</span> <span class="mi">58</span><span class="p">,</span>
 <span class="s">'W'</span><span class="p">:</span> <span class="mi">59</span><span class="p">,</span>
 <span class="s">'X'</span><span class="p">:</span> <span class="mi">60</span><span class="p">,</span>
 <span class="s">'Y'</span><span class="p">:</span> <span class="mi">61</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>
<p>영어도 있고 숫자도 있고 특수문자나 이스케이프문자(\n 등)도 포함되어 있습니다.</p>

<h4 id="케라스를-사용한-원-핫-인코딩">케라스를 사용한 원-핫 인코딩</h4>

<p>다음은 케라스 API를 사용한 원-핫 인코딩의 코드입니다. 설명은 주석으로 대체합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>

<span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="s">'The cat sat on the mat.'</span><span class="p">,</span> <span class="s">'The dog ate my homework.'</span><span class="p">]</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span> <span class="c1">#가장 빈도가 높은 단어 1000개를 사용하도록 합니다.
</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="c1">#단어인덱스를 만듭니다. tokenizer에 만들어집니다.
</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>  <span class="c1">#문장을 단어인덱스의 인덱스로 표현합니다. 첫 번째 문장의 경우 [1, 2, 3, 4, 1, 5] 이렇게 표현, 각 인덱스는 각 단어
</span>
<span class="n">one_hot_results</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">texts_to_matrix</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'binary'</span><span class="p">)</span> <span class="c1">#인덱스의 시퀀스로 표현된 문장을 원핫으로 인코딩 합니다.
</span>
<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span> <span class="c1">#단어 인덱스가 word_index에 포함되어 있습니다. 이전 코드의 token_index처럼 각 단어가 키이고 고유인덱스가 값으로 구성된 dict입니다.
</span><span class="k">print</span><span class="p">(</span><span class="s">'%s개의 고유한 토큰을 찾았습니다.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="원-핫-해싱">원-핫 해싱</h3>
<p>원-핫 인코딩의 변종인 원-핫 해싱(one-hot hashing)이 존재합니다. 이 방식은 고유한 토큰수(단어 또는 문자)가 너무 커서 토큰에 대한 백터가 너무 클때 사용됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="s">'The cat sat on the mat.'</span><span class="p">,</span> <span class="s">'The dog ate my homework.'</span><span class="p">]</span>

<span class="n">dimensionality</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1">#최대 1000개의 단어를 표현하는 백터를 의미
</span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">),</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">dimensionality</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">sample</span><span class="p">.</span><span class="n">split</span><span class="p">()))[:</span><span class="n">max_length</span><span class="p">]:</span> <span class="c1">#문장에서 최대 10개의 토큰을 표현
</span>    <span class="n">index</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="nb">hash</span><span class="p">(</span><span class="n">word</span><span class="p">))</span> <span class="o">%</span> <span class="n">dimensionality</span> <span class="c1"># token_index를 미리 생성하는것이 아니라 해시를 사용해서 index를 부여
</span>    <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">index</span><span class="p">]</span>  <span class="o">=</span> <span class="mf">1.</span>
</code></pre></div></div>
<p>최대 1000개 이지만 그 이상의 단어가 표현될 경우 충돌이 발생합니다.</p>

<h3 id="12-단어-임베딩-사용하기">1.2 단어 임베딩 사용하기</h3>
<p>단어를 백터로 표현하는 또 다른 방법은 단어 임베딩입니다. 원핫인코딩을 생각해 봅시다. 단어가 1000가지 라면 하나의 토큰을 표현하는데 1000개의 원소를 가진 백터가 필요합니다(고차원). 거기에다 단 하나의 인덱스에만 1이 입력되어 있고 나저미는 0이 채워집니다(희소). 하나의 토큰을 표현하기 위해 많은 메모리를 사용하죠. 원핫인코딩과 다르게 <strong>단어 임베딩</strong>은 고정된 백터를 사용하고 1이나 0이 아닌 실수의 집합으로 단어를 표현하는 방식입니다.</p>

<h4 id="원핫-인코딩">원핫 인코딩</h4>
<ul>
  <li>고차원</li>
  <li>희소(sparse)</li>
</ul>

<h4 id="단어임베딩">단어임베딩</h4>
<ul>
  <li>실수형 백터 사용</li>
  <li>저차원</li>
  <li>밀집(density)</li>
</ul>

<p>단어 임베딩을 만드는 2가지 방법</p>
<ul>
  <li>관심대상인 문제와 함께 단어 임베딩을 학습, 즉 문제에 표함되어 있는 단어들을 학습</li>
  <li>미리 계산된 단어 임베딩(pretrained word embedding)을 사용, 즉 일반화 되어 있는 단어집으로 학습된 단어임베딩을 사용</li>
</ul>

<p>각각의 방법에 대해 알아보겠습니다.</p>

<h4 id="케라스의-embedding층을-사용하여-단어-임베딩-학습하기">케라스의 Embedding층을 사용하여 단어 임베딩 학습하기</h4>

<p>케라스의 Embedding층을 사용하면 다음과 같이 쉽게 단어임베딩을 학습할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span>
<span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="c1">#가능한 토큰개수 1000, 임베딩 차원(토큰을 표현하는 백터의 크기) 64
</span></code></pre></div></div>

<p>Embedding층은 학습할 단어의 수(samples), 하나의 단어를 표현하는 차원수(sequence_length)를 파라미터로 받습니다. 위 코드는 총 1000개의 단어가 존재하고 하나의 단어는 64개의 실수로 표현한다는 의미입니다. 입력 데이터를 받으면 Embedding층은 ()</p>

<h4 id="사전-훈련된-단어-임베딩-사용하기">사전 훈련된 단어 임베딩 사용하기</h4>

<h3 id="13-imdb데이터-셋으로-단어-임베딩-학습하기">1.3 IMDB데이터 셋으로 단어 임베딩 학습하기</h3>
<p>여기서는 이전에 영화분류를 위해 사용했던 데이터셋(IMDB)를 가지고 단어임베딩을 학습해 보겠습니다.</p>

<h4 id="원본-imdb-텍스트-내려받기">원본 IMDB 텍스트 내려받기</h4>
<p>IMDB의 원본데이터를 http://mng.bz/0tIo 에서 다운받습니다. 본 파일은 train과 test폴도로 나누어져 있고 다시 neg와 pos 폴더로 긍/부정 데이터가 나누어져 있습니다. 여기서는 훈련데이터셋의 neg와 pos를 텍스트와 레이블의 리스트로 불러옵니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>

<span class="n">imdb_dir</span> <span class="o">=</span> <span class="s">'./datasets/aclImdb'</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">imdb_dir</span><span class="p">,</span> <span class="s">'train'</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">label_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'neg'</span><span class="p">,</span> <span class="s">'pos'</span><span class="p">]:</span>
    <span class="n">dir_name</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">label_type</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">dir_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">fname</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span> <span class="o">==</span> <span class="s">'.txt'</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">fname</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf8'</span><span class="p">)</span>
            <span class="n">texts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">())</span>
            <span class="n">f</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">label_type</span> <span class="o">==</span> <span class="s">'neg'</span><span class="p">:</span>
                <span class="n">labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#부정이면 0
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#긍정이면 1
</span></code></pre></div></div>
<p>text와 labels은 인덱스로 매칭이 됩니다.</p>

<h4 id="데이터-토큰화">데이터 토큰화</h4>
<p>여기서는 사전훈련된 단어 임베딩을 같이 사용합니다(<strong>문제에 특화된 데이터 셋이 충분히 있다면 그 데이터 셋으로 단어임베딩을 만드는 것이 문제 해결에 있어서 훨씬 성능이 좋습니다.</strong>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">maxlen</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># 텍스트 1개당 최대 100개의 단어를 사용
</span><span class="n">training_samples</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1"># 200개의 훈련 데이터 사용
</span><span class="n">validation_samples</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c1"># 검증은 1만개의 데이터 사용
</span><span class="n">max_words</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c1"># 데이터셋에서 가장 빈도가 높은 1만개의 단어사용
</span>
<span class="c1"># 데이터 셋에서 1만개의 토큰(단어)를 추출
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_words</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span> <span class="c1">#텍스트를 토큰인덱스에 대한 백터로 변환
</span>
<span class="c1"># 단어:인덱스 에 대한 딕셔너리
</span><span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>
<span class="k">print</span><span class="p">(</span><span class="s">'%s개의 고유한 토큰을 찾았습니다.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">))</span>

<span class="c1">#텍스트가 포함한 단어의 수가 maxlen보다 0으로 채움, 길면maxlen이후는 버림
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'데이터 텐서의 크기:'</span><span class="p">,</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'레이블 텐서의 크기:'</span><span class="p">,</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1">#데이터 개수에 대한 넘파이 배열(0~24999)을 랜덤하게 썩도 추출
</span><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

<span class="c1">#200개의 훈련셋, 10000개의 검증셋
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="n">training_samples</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:</span><span class="n">training_samples</span><span class="p">]</span>
<span class="n">x_val</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">training_samples</span><span class="p">:</span> <span class="n">training_samples</span> <span class="o">+</span> <span class="n">validation_samples</span><span class="p">]</span>
<span class="n">y_val</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">training_samples</span><span class="p">:</span> <span class="n">training_samples</span> <span class="o">+</span> <span class="n">validation_samples</span><span class="p">]</span>
</code></pre></div></div>

<h4 id="glove-단어-임베딩-내려받고-임베딩-불러오기">GloVe 단어 임베딩 내려받고 임베딩 불러오기</h4>
<p>https://nlp.stanford.edu/projects/glove에서 2014년에 영문 위키피디아를 이용해 사전훈련된 임베딩을 다운받습니다. 40만개의 단어와 100차원 임베딩 벡터를 포함하고 있습니다. 즉 40만 개의 토큰이 존재하고 하나의 토큰은 100개의 실수로 표현된 데이터 입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">glove_dir</span> <span class="o">=</span> <span class="s">'./datasets'</span>

<span class="n">embeddings_index</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">glove_dir</span><span class="p">,</span> <span class="s">'glove.6B.100d.txt'</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf8'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">()</span> <span class="c1">#단어 실수1 실수2 .. 실수100
</span>    <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#첫번째는 단어
</span>    <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">)</span> <span class="c1">#나머지는 100개의 실수
</span>    <span class="n">embeddings_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span>
<span class="n">f</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">'%s개의 단어 백터를 찾았습니다.'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">))</span>
</code></pre></div></div>

<p>영화리뷰데이터셋에서 뽑아온 1만개의 단어에 대한 임베딩을 GloVe 임베딩에서 가져옵니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">max_words</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">))</span> <span class="c1">#(10000, 100) 의 임베딩 행렬
</span><span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="p">.</span><span class="n">items</span><span class="p">():</span> <span class="c1">#영화리뷰에서 최대 10000개의 데이터 추출
</span>    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_words</span><span class="p">:</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embeddings_index</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="c1">#사전 학습된 Glove 임베딩에 해당 단어가 존재하면 가져오기
</span>        <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
</code></pre></div></div>

<h4 id="모델-정의">모델 정의</h4>

<p>이전 분류모델과 같은 모델입니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_words</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">maxlen</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="모델에-glove-임베딩-로드">모델에 GloVe 임베딩 로드</h4>
<p>Embedding 층은 하나의 가중치 행렬을 가집니다. 이 행렬은 입력인덱스에 대해 임베딩 결과를 출력하는 행렬입니다. 사전훈된된 결과를 주입하고 학습이 안되도록 파라미터를 설정합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_weights</span><span class="p">([</span><span class="n">embedding_matrix</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div>

<h4 id="모델-훈련과-평가">모델 훈련과 평가</h4>

<p>모델을 평가하고 결과를 시각화해 봅시다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span>
             <span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span>
             <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                   <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                   <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_val</span><span class="p">,</span><span class="n">y_val</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s">'pre_trained_glove_model.h5'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'acc'</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/public/keras/embedding_1.png" alt="" /></p>

<p><img src="/public/keras/embedding_2.png" alt="" /></p>

<p>아주 빠르게 과적합 되는것을 확인 할 수 있습니다. 이번에는 테스트 데이터에 모델을 적용해 보겠습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">imdb_dir</span><span class="p">,</span> <span class="s">'test'</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">label_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'neg'</span><span class="p">,</span> <span class="s">'pos'</span><span class="p">]:</span>
    <span class="n">dir_name</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">label_type</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">dir_name</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">fname</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]</span> <span class="o">==</span> <span class="s">'.txt'</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">fname</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf8'</span><span class="p">)</span>
            <span class="n">texts</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">())</span>
            <span class="n">f</span><span class="p">.</span><span class="n">close</span>
            <span class="k">if</span> <span class="n">label_type</span> <span class="o">==</span> <span class="s">'neg'</span><span class="p">:</span>
                <span class="n">labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">maxlen</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="c1">#모델 불러와서 평가
</span><span class="n">model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">'pre_trained_glove_model.h5'</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">evaludate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>[0.7864487828063965, 0.57648]
0.57 의 정확도가 측정되었습니다.</p>
</blockquote>


  </article>

  <hr>

  <!-- <div class="question">
    <h2>Questions?</h2>
    <p>Have a question regarding the post above? <br />Or any of my designs?</p>
    

  </div> -->

  <!-- <div class="related">
    <h2>Related</h2>
    
      <li><a href="/%EB%8F%84%EC%BB%A4/docker-aws-travis/" title="도커와 AWS기반으로 CI환경 구축하기">도커와 AWS기반으로 CI환경 구축하기
       &nbsp; <span class="post-meta">January 30, 2021</span></a>
    
      <li><a href="/%EB%8F%84%EC%BB%A4/docker-swarm/" title="도커 스웜(Docker Swarm)">도커 스웜(Docker Swarm)
       &nbsp; <span class="post-meta">January 28, 2021</span></a>
    
      <li><a href="/micro-service-from-scratch/" title="Micro Service From Scratch">Micro Service From Scratch
       &nbsp; <span class="post-meta">January 24, 2021</span></a>
    
  </div> -->

  
  
    <div id="disqus_thread"></div>
    <script>
        /**
        *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
        *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
        /*
        var disqus_config = function () {
        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        */
        (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://youngwonseo.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

</div>

        </div>
        <footer class="site-footer">
<p class="small">YOUNGWON &copy 2021
</p>
</footer>

    </div>

    <script src="//cdn.jsdelivr.net/headroomjs/0.5.0/headroom.min.js"></script>
    <script type="text/javascript">
      var el = document.querySelector(".header-container");
      var headroom  = new Headroom(el, {
        "offset": 205,
        "tolerance": 5
      });
      headroom.init();
    </script>


    <!-- Twitter Shizzle -->
    <script type="text/javascript">
    window.twttr = (function (d, s, id) {
      var t, js, fjs = d.getElementsByTagName(s)[0];
      if (d.getElementById(id)) return;
      js = d.createElement(s); js.id = id;
      js.src= "https://platform.twitter.com/widgets.js";
      fjs.parentNode.insertBefore(js, fjs);
      return window.twttr || (t = { _e: [], ready: function (f) { t._e.push(f) } });
    }(document, "script", "twitter-wjs"));
    </script>

  </body>
</html>
