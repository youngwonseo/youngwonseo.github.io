<p>강화학습을 공부하기에 앞서 순차적 결정 문제에 대해 살펴보겠습니다. 순차적 결정 문제는 MDP(Markov Decision Process)로 정의할 수 있습니다.</p>

<p>MDP에서 학습을 위해 가치함수라는 것이 도입되는데, 이는 벨만 방정식과 연결됩니다.</p>

<p>하나씩 차근히 정리해 갑시다.</p>

<h2 id="mdp">MDP</h2>

<p>강화학습은 순차적으로 행동을 결정하는 문제를 푸는것인데, 이를 수학적으로 표현한 것이 MDP입니다. 다음은 <strong>MDP의 구성요소</strong> 들입니다.</p>

<ul>
  <li>상태</li>
  <li>행동</li>
  <li>보상 함수</li>
  <li>상태 변환 확률</li>
  <li>감가율</li>
</ul>

<h3 id="상태">상태</h3>

<p>에이전트가 관찰할 수 있는 상태의 집합을 말합니다. 즉 5X5 그리드에서는 다음과 같은 상태집합을 가질 수 있습니다.</p>

<p>[수식]</p>

<p>또한 시간 t에 관찰 가능한 상태를 다음과 같이 표현합니다.</p>

<p>[수식]</p>

<h3 id="행동">행동</h3>

<p>그리드 위를 움직이는 에이전트가 할 수 있는 가능한 행동집합은 다음과 같습니다.</p>

<p>[수식]</p>

<p>또한 시간 t에서 행동집합 A에서 특정한 행동 a를 했다면 다음과 같이 표현합니다.</p>

<h3 id="보상함수">보상함수</h3>

<p>보상은 에이전트가 학습할 수 있는 유일한 정보입니다.</p>

<h3 id="상태-변환-확률">상태 변환 확률</h3>

<h3 id="감가율">감가율</h3>

<h3 id="정책">정책</h3>

<h2 id="가치함수">가치함수</h2>

<h3 id="가치함수-1">가치함수</h3>

<h3 id="큐함수">큐함수</h3>

<h2 id="벨만-방정식">벨만 방정식</h2>

<h3 id="벨만-기대-방정식">벨만 기대 방정식</h3>

<h3 id="벨만-최적-방적식">벨만 최적 방적식</h3>

